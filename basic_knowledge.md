# ë”¥ëŸ¬ë‹Â CNNÂ ëª¨ë¸Â ì•„í‚¤í…ì²˜Â ê°„ë‹¨ ì •ë¦¬Â (2014â€¯â€“â€¯2019)

---

## ëª©ì°¨
| index | ë²”ì£¼ | ìƒì„¸ ëª©ë¡ |
|-----|------|-----------|
| **1** | VGGÂ â†’Â Inception | VGGâ€‘16/19 Â· Inceptionâ€‘v1 Â· Inceptionâ€‘v2/3 |
| **2** | ResNet's | ResNetâ€‘v1/v2 Â· Inceptionâ€‘ResNet & v4 Â· Wideâ€‘ResNet Â· ResNeXt Â· DenseNet |
| **3** | íš¨ìœ¨ì ì¸ CNN êµ¬ì¡° | SEâ€‘Net Â· MobileNetâ€‘v1/v2/v3 Â· EfficientNetâ€‘B0â€¦B7 |

---

## 1Â :Â VGGÂ â†’Â Inception

### 1.Â VGGÂ NetÂ (2014â€‘09) <a id="vgg-net-201409"></a>

| êµ¬ë¶„ | ë‚´ìš© |
|------|------|
| **ì œì•ˆ ë…¼ë¬¸** | *Very Deep Convolutional Networks for Largeâ€‘Scale Image Recognition* (ICLRÂ 2015) |
| **ë¬¸ì œ ì˜ì‹** | AlexNetÂ·ZFNetì€ **ì–•ê³  í° í•„í„°**(11Ã—11Â·7Ã—7)ë¥¼ ì‚¬ìš© â†’ ì¶©ë¶„íˆ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµì— í•œê³„ & íŒŒë¼ë¯¸í„° ê³¼ë‹¤ |
| **í•µì‹¬ ì•„ì´ë””ì–´** | "**ì‘ì€ 3Ã—3 í•„í„°ë¥¼ ê¹Šê²Œ** ìŒ“ìœ¼ë©´ ë™ì¼ ìˆ˜ìš©ì˜ì—­(receptive field)ì„ ë” ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ì–»ì„ ìˆ˜ ìˆë‹¤" |
| **ì•„í‚¤í…ì²˜** | `Conv3Ã—3Ã—{2,2,3,3,3} + MaxPool` 5ë¸”ë¡ â†’ FC(4096)Ã—2 â†’ Softmax (ì´Â 16/19Â layer) |
| **ì„±ê³¼** | ImageNetÂ Topâ€‘5Â ì—ëŸ¬Â 7.3â€¯%(VGGâ€‘16) â€” ë‹¹ì‹œ 2ìœ„, ëª¨ë¸ ë‹¨ìˆœì„±ìœ¼ë¡œ ì´í›„ transferÂ learning í‘œì¤€ |
| **í•œê³„** | 138â€¯MÂ íŒŒë¼ë¯¸í„°Â·15.5â€¯GFLOPs â†’ ì¶”ë¡ Â·ë©”ëª¨ë¦¬Â ë¹„ìš© ë†’ìŒ |

```mermaid
flowchart TD
    In[224Ã—224 RGB] --> B1[3Ã—3 ConvÃ—2â†’64] --> P1
    P1 --> B2[3Ã—3 ConvÃ—2â†’128] --> P2
    P2 --> B3[3Ã—3 ConvÃ—3â†’256] --> P3
    P3 --> B4[3Ã—3 ConvÃ—3â†’512] --> P4
    P4 --> B5[3Ã—3 ConvÃ—3â†’512] --> GAP --> FC1[4096] --> FC2[4096] --> FC3[1000]
```

```python
class VGGBlock(nn.Module):
    """3Ã—3 Conv ë‘Â ë²ˆÂ + MaxPool"""
    def __init__(self, in_c, out_c):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_c, out_c, 3, padding=1), nn.ReLU(inplace=True),
            nn.Conv2d(out_c, out_c, 3, padding=1), nn.ReLU(inplace=True),
            nn.MaxPool2d(2))
    def forward(self,x):
        return self.block(x)
```

---

### 2.Â InceptionÂ v1Â (GoogLeNet,Â 2014â€‘09) <a id="inception-v1-201409"></a>

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì œì•ˆ ë…¼ë¬¸** | *Going Deeper with Convolutions* (CVPRÂ 2015) |
| **ë°°ê²½** | ìµœì  í•„í„° í¬ê¸°(1Ã—1?Â 3Ã—3?Â 5Ã—5?) ì‚¬ì „ ê²°ì • ì–´ë ¤ì›€ & í° í•„í„°Â =Â ì—°ì‚° í­ì¦ ë¬¸ì œ |
| **í•µì‹¬ ì•„ì´ë””ì–´** | í•œ "**InceptionÂ Module**"ì—ì„œ 1Ã—1Â·3Ã—3Â·5Ã—5 Conv, 3Ã—3 MaxPool **ë³‘ë ¬** ìˆ˜í–‰ â†’ ì±„ë„Â `concat` |
| **ì—°ì‚° ìµœì í™”** | 3Ã—3Â·5Ã—5 ì•ë‹¨ì— **1Ã—1Â Dimâ€‘Reduction**(ì±„ë„ 1/4Â ì¶•ì†Œ) â†’ FLOPs ëŒ€í­ ì ˆê° |
| **ë³´ì¡° ë¶„ë¥˜ê¸°** | ì¤‘ê°„ ì¶œë ¥ì— AuxiliaryÂ ClassifierÂ 2ê°œ â†’Â ì´ˆê¸° í•™ìŠµÂ ì•ˆì •í™” |
| **íŒŒë¼ë¯¸í„° ìˆ˜** | 13â€¯MÂ (<Â VGGÂ 10%) |

```python
class InceptionModule(nn.Module):
    def __init__(self,in_c,c1,reduce3,c3,reduce5,c5,proj):
        super().__init__()
        self.p1 = nn.Conv2d(in_c,c1,1)
        self.p2 = nn.Sequential(nn.Conv2d(in_c,reduce3,1),nn.ReLU(True),
                                nn.Conv2d(reduce3,c3,3,padding=1))
        self.p3 = nn.Sequential(nn.Conv2d(in_c,reduce5,1),nn.ReLU(True),
                                nn.Conv2d(reduce5,c5,5,padding=2))
        self.p4 = nn.Sequential(nn.MaxPool2d(3,1,1),nn.Conv2d(in_c,proj,1))
    def forward(self,x):
        outs=[self.p1(x),self.p2(x),self.p3(x),self.p4(x)]
        return torch.cat(outs,1)
```

---

### 3.Â InceptionÂ v2Â &Â v3Â (2015â€‘12) <a id="inception-v2--v3-201512"></a>

| ê°œì„  | ì„¤ëª… |
|------|------|
| **BatchÂ Norm** | ëª¨ë“  ConvÂ ë’¤ BN â†’ í•™ìŠµ ì•ˆì •í™” (BNâ€‘Inception) |
| **FactorizedÂ Conv** | 5Ã—5Â â†’Â 3Ã—3Ã—2,Â 3Ã—3Â â†’Â (1Ã—3Â +Â 3Ã—1) â†’Â ì—°ì‚°ëŸ‰Â â†“ |
| **LabelÂ Smoothing** | Oneâ€‘hotÂ â†’Â `[0.9,Â 0.1/(K-1)]`Â ë¶„í¬ â†’Â ê³¼ì í•©Â â†“ |

```python
def cross_entropy_ls(logits, tgt, eps=0.1):
    K = logits.size(1)
    smooth = torch.full_like(logits, eps/(K-1))
    smooth.scatter_(1, tgt.unsqueeze(1), 1-eps)
    return -(smooth * F.log_softmax(logits,1)).sum(1).mean()
```

---

## 2Â :Â ResNetÂ íŒ¨ë°€ë¦¬

### 4.Â ResNetÂ (2015â€‘12) <a id="resnet-201512"></a>

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì œì•ˆ ë…¼ë¬¸** | *Deep Residual Learning for Image Recognition* (CVPRÂ 2016) |
| **ë¬¸ì œ ì˜ì‹** | ê¹Šì´ ì¦ê°€ ì‹œ **degradation**(í›ˆë ¨Â·í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ ë™ë°˜ ìƒìŠ¹) â†’ í•­ë“± í•¨ìˆ˜ í•™ìŠµ ì–´ë ¤ì›€ |
| **í•µì‹¬ ì•„ì´ë””ì–´** | ì¸µì´ ì§ì ‘ **ì”ì°¨ F(x)=H(x)â€“x** ë¥¼ í•™ìŠµ & **skipÂ connection**ìœ¼ë¡œ `y=F(x)+x` |
| **ë¸”ë¡ ì¢…ë¥˜** | *Basic*Â (3Ã—3Â ConvÃ—2,Â CIFAR) Â· *Bottleneck*Â (1Ã—1â†’3Ã—3â†’1Ã—1,Â ImageNet) |
| **ëŒ€í‘œ ëª¨ë¸** | ResNetâ€‘18/34/50/101/152Â (Topâ€‘5Â ì—ëŸ¬Â 3.57â€¯%,Â 152â€‘layer) |

```python
class Bottleneck(nn.Module):
    exp=4
    def __init__(self,in_c,mid,stride=1):
        super().__init__()
        out_c = mid*self.exp
        self.conv = nn.Sequential(
            nn.Conv2d(in_c,mid,1,bias=False), nn.BatchNorm2d(mid), nn.ReLU(True),
            nn.Conv2d(mid,mid,3,stride,1,bias=False), nn.BatchNorm2d(mid), nn.ReLU(True),
            nn.Conv2d(mid,out_c,1,bias=False), nn.BatchNorm2d(out_c))
        self.down = nn.Identity() if stride==1 and in_c==out_c else \
            nn.Sequential(nn.Conv2d(in_c,out_c,1,stride,bias=False), nn.BatchNorm2d(out_c))
    def forward(self,x):
        return F.relu(self.conv(x) + self.down(x))
```

#### 4â€‘1.Â Preâ€‘ActivationÂ ResNetÂ (v2,Â 2016â€‘03)
> BNÂ â†’Â ReLUÂ â†’Â Conv ìˆœìœ¼ë¡œ ë³€ê²½í•´ **í•­ë“± ê²½ë¡œ**ë¥¼ ê¹¨ë—ì´ ìœ ì§€, 1000â€‘layerÂ CIFARÂ ì‹¤í—˜ ì„±ê³µ.

#### 4â€‘2.Â ResNetÂ =Â EnsembleÂ (VeitÂ 2016)
> skip ë•ë¶„ì— ì…ë ¥â†’ì¶œë ¥ **ê²½ë¡œ ìˆ˜ = 2á´¸**, ì–•ì€ ë„¤íŠ¸ì›Œí¬ë“¤ì˜ ì•™ìƒë¸”ì²˜ëŸ¼ ë™ì‘í•œë‹¤ëŠ” í•´ì„.

#### 4â€‘3.Â LossÂ LandscapeÂ ì‹œê°í™”Â (LiÂ 2018)
> ì”ì°¨ ì—°ê²°ì´ ì—†ëŠ” ë§ì€ ì¢ê³  ë¶ˆê·œì¹™í•œ ìµœì†Œê°’ vs. ResNetì€ **ë„“ê³  í‰íƒ„í•œ minima**.

---

### 5.Â InceptionÂ v4 & Inceptionâ€‘ResNetÂ v1/v2Â (2016â€‘02) <a id="inception-v4--inception-resnet-201602"></a>

* Inceptionâ€‘v3 ëª¨ë“ˆì„ **ì •ì œ + ë” ê¹Šê²Œ** (A/B/CÂ ëª¨ë“ˆ)
* **Inceptionâ€‘ResNet**Â : ì¸ì…‰ì…˜Â ëª¨ë“ˆ ì¶œë ¥Â +Â ì…ë ¥ **ResidualÂ sum**
* ì±„ë„ 1000â†‘ì—ì„œ ë¶ˆì•ˆì • â†’ `0.1Ã—F(x)` **ResidualÂ Scaling**ìœ¼ë¡œ í•´ê²°

---

### 6.Â WideÂ ResNetÂ (2016â€‘05) <a id="wide-resnet-201605"></a>

| ìš”ì†Œ | ìƒì„¸ |
|------|-----|
| **ì•„ì´ë””ì–´** | ê¹Šì´ ëŒ€ì‹  **í­ k ë°°** í™•ì¥ (WRNâ€‘dâ€‘k) |
| **ëŒ€í‘œ** | WRNâ€‘28â€‘10 : 28â€‘layerÂ·k=10 â†’ ResNetâ€‘1001 ì„±ëŠ¥, í•™ìŠµ 8Ã— ë¹ ë¦„ |
| **íŠ¸ë¦­** | ë¸”ë¡ ë‚´ë¶€ **Dropout 0.3** â†’ ë„“ì–´ì§„ ëª¨ë¸ ê³¼ì í•© ì™„í™” |

---

### 7.Â ResNeXtÂ (2016â€‘11) <a id="resnext-201611"></a>

* **Aggregated Transformations** : ê·¸ë£¹ ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ **Cardinality C** ì¦ê°€
* ResNeXtâ€‘50Â **32Ã—4d**Â (C=32, groupë§ˆë‹¤ 4ì±„ë„) â†’ ResNetâ€‘50 ëŒ€ë¹„ +1.7â€¯%Â Topâ€‘1

```python
# 3Ã—3Â GroupÂ Conv ì˜ˆì‹œ (64â†’64, groups=32)
conv_g = nn.Conv2d(64,Â 64,Â 3,Â 1,Â 1,Â groups=32)
```

---

### 8.Â DenseNetÂ (2016â€‘08) <a id="densenet-201608"></a>

* ëª¨ë“  ì´ì „ ì¶œë ¥ `concat` â†’ **íŠ¹ì„± ì¬ì‚¬ìš©**Â·ê¸°ìš¸ê¸° íë¦„ ì›í™œÂ·íŒŒë¼ë¯¸í„° íš¨ìœ¨
* `growthÂ rate k` : ë§¤ ë ˆì´ì–´ ì¶”ê°€ ì±„ë„ (k=32)
* DenseNetâ€‘121Â : 8â€¯MÂ paramsë¡œ ResNetâ€‘50 ì„±ëŠ¥

```python
for layer in layers:
    x = torch.cat([x, layer(x)], dim=1)  # Dense connectivity
```

---

## Â 3Â :Â EfficientÂ CNN

### 9.Â SEâ€‘NetÂ (2017â€‘09) <a id="se-net-201709"></a>

| ë‹¨ê³„ | ì—°ì‚° |
|------|-----|
| **Squeeze** |  `z_c = 1/HW Î£_{i,j} x_{c,i,j}`Â (GlobalÂ AvgPool) |
| **Excitation** | 2Ã—FC (`C â†’ C/r â†’ C`) +Â sigmoid â†’ ê°€ì¤‘ì¹˜ `s_c` |
| **Scale** | `y_c = s_c Â· x_c` |

> SEâ€‘ResNeXtâ€‘101Â â†’Â ImageNetÂ Topâ€‘5Â 2.25â€¯%(2017Â ìš°ìŠ¹)

---

### 10â€‘12.Â MobileNetÂ ì‹œë¦¬ì¦ˆÂ (2017â€‘04Â â†’Â 2019â€‘05) <a id="mobilenet-ì‹œë¦¬ì¦ˆ-201704--201905"></a>

| ë²„ì „ | êµ¬ì¡° | íŠ¹ì§• |
|------|------|------|
| **v1** | `[DWÂ 3Ã—3 + PWÂ 1Ã—1]` ë°˜ë³µ | DepthwiseÂ SeparableÂ Convë¡œ FLOPsÂ â‰ˆÂ 1/9 |
| **v2** | `Inverted Residual (1Ã—1Â Expand â†’Â DWÂ 3Ã—3 â†’Â 1Ã—1Â Linear)` | skipÂ ì—°ê²°ì€ ì¢ì€ ì…ë ¥/ì¶œë ¥ì—, **ReLU6** ì‚¬ìš© |
| **v3** | v2Â +Â **NAS(MNASNet) íƒìƒ‰**, **hâ€‘swish**, **SE** | Large / Small ë‘Â config |

```python
# v2Â Inverted Residual ë¸”ë¡
class InvertedRes(nn.Module):
    def __init__(self,in_c,out_c,exp=6,stride=1):
        super().__init__()
        hid = in_c*exp
        self.use_skip = stride==1 and in_c==out_c
        layers=[nn.Conv2d(in_c,hid,1,bias=False), nn.BatchNorm2d(hid), nn.ReLU6(True),
                 nn.Conv2d(hid,hid,3,stride,1,groups=hid,bias=False), nn.BatchNorm2d(hid), nn.ReLU6(True),
                 nn.Conv2d(hid,out_c,1,bias=False), nn.BatchNorm2d(out_c)]
        self.conv = nn.Sequential(*layers)
    def forward(self,x):
        out = self.conv(x)
        return x+out if self.use_skip else out
```

---

### 13.Â EfficientNetÂ (2019â€‘05) <a id="efficientnet-201905"></a>

| ìš”ì†Œ | ì„¤ëª… |
|------|------|
| **BaselineÂ B0** | NASë¡œ ì„¤ê³„ëœ MobileNetV2+SE ê¸°ë°˜ **MBConv** ë¸”ë¡ |
| **Compound Scaling** | ë‹¨ì¼Â Ï†ì— ëŒ€í•´ ê¹Šì´Â Î±^Ï†, í­Â Î²^Ï†, í•´ìƒë„Â Î³^Ï† (Î±Â·Î²Â²Â·Î³Â² â‰ˆÂ 2) |
| **ì‹œë¦¬ì¦ˆ** | B0Â (224Â²,Â 5â€¯M) â†’ â€¦ â†’Â B7Â (600Â²,Â 66â€¯M) Topâ€‘1Â 84.4â€¯% |

```python
# MBConv ì˜ì‚¬ ì½”ë“œ
x = act(BN(expand_conv(x)))   # 1Ã—1Â expand
x = act(BN(DW_conv(x)))       # depthwiseÂ 3Ã—3/5Ã—5
x = se(x)                     # Squeezeâ€‘Excite
x = BN(project_conv(x))       # 1Ã—1Â project
out = x+input if stride==1 and in==out else x
```

---

## ì°¸ê³ Â ë¬¸í—Œ Â· ì¶”ì²œ ë¦¬ë”©
- **VGG**Â : Simonyan & Zisserman,Â *Very Deep ConvNets*,Â ICLRÂ 2015
- **Inception ì‹œë¦¬ì¦ˆ**Â : SzegedyÂ etÂ al.,Â *Going/Rethinking/Inceptionâ€‘ResNet*,Â CVPRÂ 2015â€‘16
- **ResNet**Â : HeÂ etÂ al.,Â *Deep Residual Learning*,Â CVPRÂ 2016
- **ResNetâ€‘v2**Â : HeÂ etÂ al.,Â *Identity Mappings*,Â ECCVÂ 2016
- **ResNeXt**Â : XieÂ etÂ al.,Â *Aggregated Residual Transformations*,Â CVPRÂ 2017
- **DenseNet**Â : HuangÂ etÂ al.,Â *Densely Connected CNN*,Â CVPRÂ 2017
- **SENet**Â : HuÂ etÂ al.,Â *Squeezeâ€‘andâ€‘Excitation Networks*,Â CVPRÂ 2018
- **MobileNetV3**Â : HowardÂ etÂ al.,Â *Searching for MobileNetV3*,Â ICCVÂ 2019
- **EfficientNet**Â : TanÂ &Â Le,Â *EfficientNet*,Â ICMLÂ 2019

> ğŸ’¡ í˜íœí•˜ì„ë‹˜ legend 13 ê°•ì˜ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë“¤ì„ ê°„ë‹¨ ì •ë¦¬í•œ ìë£Œì…ë‹ˆë‹¤.

